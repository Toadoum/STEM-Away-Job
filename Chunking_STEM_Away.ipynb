{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Toadoum/STEM-Away-Job/blob/master/Chunking_STEM_Away.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "trSlIMT_U8B1"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np \n",
        "import re\n",
        "import nltk\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mlb5G5izVul3",
        "outputId": "4389fefe-ca31-49da-b91f-482b37f2cc2a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xxobNld2eYlU",
        "outputId": "e8e2d486-045b-46ae-f1c8-c21474d90778"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1006 entries, 0 to 1005\n",
            "Data columns (total 9 columns):\n",
            " #   Column              Non-Null Count  Dtype \n",
            "---  ------              --------------  ----- \n",
            " 0   Description         1006 non-null   object\n",
            " 1   lower_description   1006 non-null   object\n",
            " 2   word_tokenized      1006 non-null   object\n",
            " 3   sentence_tokenized  1006 non-null   object\n",
            " 4   word_count          1006 non-null   int64 \n",
            " 5   sentence_count      1006 non-null   int64 \n",
            " 6   clean_words         1006 non-null   object\n",
            " 7   clean_stemmed       1006 non-null   object\n",
            " 8   clean_lemmed        1006 non-null   object\n",
            "dtypes: int64(2), object(7)\n",
            "memory usage: 70.9+ KB\n"
          ]
        }
      ],
      "source": [
        "df2 = pd.read_csv('/content/drive/My Drive/STEMAway/df_description_processed.csv')\n",
        "df2.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mrZ8od8-AVFe",
        "outputId": "e45b670d-7170-4254-9a7d-9d734c0e9889"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(992, 9)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "# duplicates reduce the df by 651 observations\n",
        "df2.drop_duplicates('lower_description', inplace= True)\n",
        "df2.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KMz98Ii7FhO2"
      },
      "source": [
        "# POS "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "84nC3dhEePhF"
      },
      "outputs": [],
      "source": [
        "def pos_series(keyword):\n",
        "    '''categorizes after tokenizing words with POS tags'''\n",
        "    tokens = nltk.word_tokenize(keyword)\n",
        "    tagged = nltk.pos_tag(tokens)\n",
        "    return tagged"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('popular')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2uOKLC2oRpV5",
        "outputId": "fe80e51e-5dfd-488e-fa93-d4f4b82a3cf5"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading collection 'popular'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/names.zip.\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet2021 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet31 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/words.zip.\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Package punkt is already up-to-date!\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection popular\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "ha3ngJ7NeP4v"
      },
      "outputs": [],
      "source": [
        "# cell runs slower due to computational exhaustion; gpu not active\n",
        "pos_tagged_arrs = df2.lower_description.apply(pos_series)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "LMjUNxnReQT4"
      },
      "outputs": [],
      "source": [
        "# unloads the tuples from the tree object for easier manipulation\n",
        "pos_tagged = []\n",
        "for row in pos_tagged_arrs.values:\n",
        "    for element in row:\n",
        "        pos_tagged.append(element)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "PM4QXbWNeviQ"
      },
      "outputs": [],
      "source": [
        "# dataframe contains all of the words with their corresponding pos tag;\n",
        "pos_df = pd.DataFrame(pos_tagged, columns = ('word','POS'))\n",
        "# special chars were removed due to irrelevance as a tag but will be included in regex\n",
        "char_removal = [',', '.', ':', '#', '$', '\\'\\'', '``', '(', ')']\n",
        "drop_indices = (pos_df.loc[pos_df.POS.isin(char_removal)].index)\n",
        "pos_df.drop(drop_indices, inplace = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "1R-tCsqAVSqT"
      },
      "outputs": [],
      "source": [
        "# print out of the following tags for easy reference\n",
        "# for tag in pos_df.POS.unique():\n",
        "#     print(nltk.help.upenn_tagset(tag))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AeEReq61HT1r"
      },
      "source": [
        "# Noun Prase #1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "q1n4F_64VS7q"
      },
      "outputs": [],
      "source": [
        "# defining grammer within the text using reg expressions\n",
        "# optional determinate, any number of adjectives, a noun, noun plural, proper noun with additionals following\n",
        "grammar1 = ('''Noun Phrases: {<DT>?<JJ>*<NN|NNS|NNP>+}''')\n",
        "chunkParser = nltk.RegexpParser(grammar1)\n",
        "tree1 = chunkParser.parse(pos_tagged)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "EhQm0Y6EYG8T"
      },
      "outputs": [],
      "source": [
        "# typical noun phrase pattern to be pickled for later analyses\n",
        "g1_chunks = []\n",
        "for subtree in tree1.subtrees(filter=lambda t: t.label() == 'Noun Phrases'):\n",
        "    # print(subtree)\n",
        "    g1_chunks.append(subtree)\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "Mqb7BCe2mr95"
      },
      "outputs": [],
      "source": [
        "with open('/content/drive/My Drive/STEMAway/chunks_1.pickle', 'wb') as fp1:\n",
        "    pickle.dump(g1_chunks, fp1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N8AsEdg8Hdpo"
      },
      "source": [
        "# Noun Phrase #2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "C7AuWWmCih7w"
      },
      "outputs": [],
      "source": [
        "# Noun phrase variation \n",
        "# preposition maybe, any number of adjective or nouns, any plural nouns or singular nouns\n",
        "grammar2 = ('''NP2: {<IN>?<JJ|NN>*<NNS|NN>} ''')\n",
        "chunkParser = nltk.RegexpParser(grammar2)\n",
        "tree2 = chunkParser.parse(pos_tagged)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "lZtufY7eYKwJ"
      },
      "outputs": [],
      "source": [
        "# variation of a noun phrase pattern to be pickled for later analyses\n",
        "g2_chunks = []\n",
        "for subtree in tree2.subtrees(filter=lambda t: t.label() == 'NP2'):\n",
        "    # print(subtree)\n",
        "    g2_chunks.append(subtree)\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "KQ8ApP1fmyod"
      },
      "outputs": [],
      "source": [
        "with open('/content/drive/My Drive/STEMAway/chunks_2.pickle', 'wb') as fp2:\n",
        "    pickle.dump(g2_chunks , fp2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HUPMERSrHhiF"
      },
      "source": [
        "## Verb-Noun Pattern #3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "vLkcIxO0VTO9"
      },
      "outputs": [],
      "source": [
        "# any sort of verb followed by any number of nouns\n",
        "grammar3 = ('''\n",
        "    VS: {<VBG|VBZ|VBP|VBD|VB|VBN><NNS|NN>*}\n",
        "    ''')\n",
        "chunkParser = nltk.RegexpParser(grammar3)\n",
        "tree3 = chunkParser.parse(pos_tagged)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "pR3EFTcAjN1C"
      },
      "outputs": [],
      "source": [
        "# verb-noun pattern to be pickled for later analyses\n",
        "g3_chunks = []\n",
        "for subtree in tree3.subtrees(filter=lambda t: t.label() == 'VS'):\n",
        "    # print(subtree)\n",
        "    g3_chunks.append(subtree)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "UooPLPB0m6p_"
      },
      "outputs": [],
      "source": [
        "with open('/content/drive/My Drive/STEMAway/chunks_3.pickle', 'wb') as fp3:\n",
        "    pickle.dump(g3_chunks, fp3)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ebFXJ-PQHmqh"
      },
      "source": [
        "# <noun, noun*> Pattern #4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "YmrbtjenVTfR"
      },
      "outputs": [],
      "source": [
        "# any number of a singular or plural noun followed by a comma followed by the same noun, noun, noun pattern\n",
        "grammar4 = ('''\n",
        "    Commas: {<NN|NNS>*<,><NN|NNS>*<,><NN|NNS>*} \n",
        "    ''')\n",
        "chunkParser = nltk.RegexpParser(grammar4)\n",
        "tree4 = chunkParser.parse(pos_tagged)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "4WOwZEb8kHLl"
      },
      "outputs": [],
      "source": [
        "# common pattern of listing skills to be pickled for later analyses\n",
        "g4_chunks = []\n",
        "for subtree in tree4.subtrees(filter=lambda t: t.label() == 'Commas'):\n",
        "    # print(subtree)\n",
        "    g4_chunks.append(subtree)\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "Krjr8UmgnYnS"
      },
      "outputs": [],
      "source": [
        "with open('/content/drive/My Drive/STEMAway/chunks_4.pickle', 'wb') as fp4:\n",
        "    pickle.dump(g4_chunks, fp4)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FxnKWHBIR1k6"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}