{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOimSkiZwyZmXO5H7JM4t1p",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Toadoum/STEM-Away-Job/blob/master/training_skills_extraction_STEM_Away.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xCTCFZMlgQ22",
        "outputId": "b9b0a9c4-344d-41d2-86ce-27bfaac4a942"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NmN3kcVMfh9_",
        "outputId": "12c46d03-0f70-4306-9c5f-ae0f166a8b9e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length: 128915 Sample Size: 12891.5\n",
            "Length: 130799 Sample Size: 13079.900000000001\n",
            "Length: 65551 Sample Size: 6555.1\n",
            "Length: 6484 Sample Size: 648.4000000000001\n",
            "'train_skills.csv' has been created\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import pickle\n",
        "\n",
        "# chunks were taken from regex of POS tags located on google colab\n",
        "chunks1 = pickle.load( open('/content/drive/My Drive/STEMAway/chunks_1.pickle', \"rb\" ) )\n",
        "chunks2 = pickle.load( open('/content/drive/My Drive/STEMAway/chunks_2.pickle', \"rb\" ) )\n",
        "chunks3 = pickle.load( open('/content/drive/My Drive/STEMAway/chunks_3.pickle', \"rb\" ) )\n",
        "chunks4 = pickle.load( open('/content/drive/My Drive/STEMAway/chunks_4.pickle', \"rb\" ) )\n",
        "\n",
        "# Sample size is 10% and will be labeled accordingly\n",
        "# perhaps a sample of a sample can be used depends on NN model\n",
        "print('Length:', len(chunks1), 'Sample Size:', len(chunks1) * .10)\n",
        "print('Length:', len(chunks2), 'Sample Size:', len(chunks2) * .10) \n",
        "print('Length:', len(chunks3), 'Sample Size:', len(chunks3) * .10)\n",
        "print('Length:', len(chunks4), 'Sample Size:', len(chunks4) * .10)\n",
        "\n",
        "def training_set(chunks):\n",
        "    '''creates a dataframe that easily parsed with the chunks data '''\n",
        "    df = pd.DataFrame(chunks)    \n",
        "    df.fillna('X', inplace = True)\n",
        "    \n",
        "    train = []\n",
        "    for row in df.values:\n",
        "        phrase = ''\n",
        "        for tup in row:\n",
        "            # needs a space at the end for seperation\n",
        "            phrase += tup[0] + ' '\n",
        "        phrase = ''.join(phrase)\n",
        "        # could use padding tages but encoder method will provide during \n",
        "        # tokenizing/embeddings; X can replace paddding for now\n",
        "        train.append( phrase.replace('X', '').strip())\n",
        "\n",
        "    df['phrase'] = train\n",
        "\n",
        "    # only returns 10% of each dataframe to be used \n",
        "    return df.phrase.sample(frac = 0.1)\n",
        "\n",
        "# one training corpus with 10% of each POS regex identification\n",
        "training = pd.concat([training_set(chunks1),\n",
        "                      training_set(chunks2), \n",
        "                      training_set(chunks3),\n",
        "                      training_set(chunks4)], \n",
        "                        ignore_index = True )\n",
        "\n",
        "training.to_csv('train_skills.csv')\n",
        "print(\"'train_skills.csv' has been created\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KqXiHFNwgHQ3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}